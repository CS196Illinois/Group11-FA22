{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports the data set arrays. the arrays are the 2d array representation of the images, pre-processed to be grayscale and 180x180 pixels.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nondemented_array = np.load(r\"AugmentedAlzheimerDataset\\NonDemented.npy\", allow_pickle = True)\n",
    "verymilddemented_array = np.load(r\"AugmentedAlzheimerDataset\\VeryMildDemented.npy\", allow_pickle = True)\n",
    "milddemented_array = np.load(r\"AugmentedAlzheimerDataset\\MildDemented.npy\", allow_pickle = True)\n",
    "moderatedemented_array = np.load(r\"AugmentedAlzheimerDataset\\ModerateDemented.npy\", allow_pickle = True)\n",
    "\n",
    "train_size = int(min(len(nondemented_array), len(verymilddemented_array), len(milddemented_array), len(moderatedemented_array)) / 2)\n",
    "#the size of each sample size in the training batch is half of the minimum sample size\n",
    "\n",
    "\n",
    "training_data = np.concatenate((nondemented_array[0: train_size], verymilddemented_array[0: train_size], milddemented_array[0: train_size], moderatedemented_array[0: train_size]))\n",
    "testing_data = np.concatenate((nondemented_array[train_size:], verymilddemented_array[train_size:], milddemented_array[train_size:], moderatedemented_array[train_size:]))\n",
    "#creates the training data by concatenating all of the arrays of length train_size together, creates testing data by concatenating the rest of the data\n",
    "\n",
    "\n",
    "np.random.shuffle(training_data)\n",
    "np.random.shuffle(testing_data)\n",
    "#shuffles the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "        self.pool1 = nn.MaxPool2d((2, 2))\n",
    "        self.pool2 = nn.MaxPool2d((2, 2))\n",
    "        self.pool3 = nn.MaxPool2d((2, 2))\n",
    "        self.fc1 = nn.Linear(46208, 46208) #size fo the fully connected layer was determined by the size of the pictures, which are 180 x 180 pixels\n",
    "        self.fc2 = nn.Linear(46208, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.flatten(start_dim=1) # flattening out\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "training_picture = torch.Tensor([i[0] for i in training_data]).view(-1, 180, 180)\n",
    "training_picture = training_picture/255.0\n",
    "training_class = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "testing_picture = torch.Tensor([i[0] for i in testing_data]).view(-1, 180, 180)\n",
    "testing_picture = testing_picture/255.0\n",
    "testing_class = torch.Tensor([i[1] for i in testing_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in (range(0, len(training_picture), BATCH_SIZE)):\n",
    "        batch_pic = training_picture[i: i + BATCH_SIZE].view(-1, 1, 180, 180)\n",
    "        batch_class = train_class[i: i + BATCH_SIZE]\n",
    "        \n",
    "        net.zero_grad()\n",
    "        outputs = net(batch_pic)\n",
    "        loss = loss_function(outputs, batch_class)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for i in (range(len(testing_picture))):\n",
    "        real_class = torch.argmax(testing_class[i])\n",
    "        net_out = net(testing_picture[i].view(-1, 1, 50, 50))[0]\n",
    "        predicted_class = torch.argmax(net_out)\n",
    "        if predicted_class == real_class:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "print(round(correct/total, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e820f3ddc028a719ffe50e7d80dd01658ce1fe998d4f6f388d9b09d11d3d164"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
